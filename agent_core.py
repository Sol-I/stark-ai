from openai import OpenAI
import logging
import asyncio
import time
from config import OPENROUTER_API_KEY, MODEL_RANKING

logging.basicConfig(level=logging.WARNING)


class AIAgent:
    def __init__(self, api_key: str = OPENROUTER_API_KEY):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.conversations = {}
        self.max_history = 3
        self.model_ranking = MODEL_RANKING
        self.current_model_index = 0
        self.last_request_time = 0
        self.request_delay = 4  # –ó–∞–¥–µ—Ä–∂–∫–∞ 4 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    async def rate_limit_delay(self):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.request_delay:
            sleep_time = self.request_delay - time_since_last_request
            logging.info(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {sleep_time:.2f} —Å–µ–∫ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º")
            await asyncio.sleep(sleep_time)

        self.last_request_time = time.time()

    async def check_model_availability(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
        try:
            await self.rate_limit_delay()  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π

            test_message = [{"role": "user", "content": "test"}]
            completion = self.client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "http://94.228.123.86:8000",
                    "X-Title": "Stark AI",
                },
                model=model_name,
                messages=test_message,
                max_tokens=10
            )
            return True
        except Exception:
            return False

    async def process_message(self, user_id: str, message: str) -> str:
        if user_id not in self.conversations:
            self.conversations[user_id] = []

        current_history = self.conversations[user_id][-self.max_history:]
        current_history.append({"role": "user", "content": message})

        try:
            await self.rate_limit_delay()  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –æ—Å–Ω–æ–≤–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º

            current_model = self.model_ranking[self.current_model_index]

            # –ü—Ä–æ–±—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            try:
                completion = self.client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "http://94.228.123.86:8000",
                        "X-Title": "Stark AI",
                    },
                    model=current_model["name"],
                    messages=current_history,
                    max_tokens=1024
                )

                ai_response = completion.choices[0].message.content
                model_info = f"\n\n---\n*–û—Ç–≤–µ—á–∞–µ—Ç {current_model['provider']} ({current_model['params']}B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)*"

            except Exception as e:
                # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—â–µ–º –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
                best_model_name = await self.find_best_available_model()
                best_model = self.model_ranking[self.current_model_index]

                completion = self.client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "http://94.228.123.86:8000",
                        "X-Title": "Stark AI",
                    },
                    model=best_model_name,
                    messages=current_history,
                    max_tokens=1024
                )

                ai_response = completion.choices[0].message.content
                model_info = f"\n\n---\n*–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ {best_model['provider']} ({best_model['params']}B)*"

            current_history.append({"role": "assistant", "content": ai_response})
            self.conversations[user_id] = current_history[-self.max_history:]

            return ai_response + model_info

        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}"

    async def find_best_available_model(self) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.model_ranking):
            if await self.check_model_availability(model["name"]):
                self.current_model_index = i
                print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª—å: {model['name']} ({model['params']}B)")
                return model["name"]

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–±–æ—Ç–∞—é—â—É—é
        return self.model_ranking[-1]["name"]

    async def background_model_checker(self):
        """–§–æ–Ω–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        while True:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –≤—ã—Å–æ–∫–∏–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º
                for i in range(3):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-3 –º–æ–¥–µ–ª–∏
                    if i != self.current_model_index and await self.check_model_availability(
                            self.model_ranking[i]["name"]):
                        if i < self.current_model_index:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Ç–µ–∫—É—â–µ–π
                            self.current_model_index = i
                            print(f"üéØ –í–µ—Ä–Ω—É–ª—Å—è –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {self.model_ranking[i]['name']}")
                            break

                await asyncio.sleep(300)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            except Exception:
                await asyncio.sleep(60)

    def clear_history(self, user_id: str):
        if user_id in self.conversations:
            del self.conversations[user_id]